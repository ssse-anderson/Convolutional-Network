{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#依次实现章节里面所有的程序，并做出相应的注释\n",
    "#每个小部分会先介绍对这部分的内容的理解，然后对关键的语句进行相应的注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2d57b619db62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#优点：结构简单，便于实现\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#缺点：学习效果较差，且计算量很大\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvPoolLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullyConnectedLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Convolutional-Network/network3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Third-party libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "###001\n",
    "\"\"\"\"\n",
    "#首先是最简单的全连接网络，仅仅使用一个隐藏层，网络只有输入层，每个像素对应一个权值参数，映射到全连接层，全连接层通过sigmoid得到输出\n",
    "#优点：结构简单，便于实现\n",
    "#缺点：学习效果较差，且计算量很大\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "\n",
    "#获取数据\n",
    "training_data, validation_data, test_data = network3.load_data_shared() \n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "#设置网络结构，输入为图像的所有像素点28x28=784,隐藏层神经元为100个\n",
    "net = Network([\n",
    "    FullyConnectedLayer(n_in=784, n_out=100),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "#用SGD算法训练数据，training_data为训练集，validation_data为验证结果，test_data为测试结果，学习率为0.1\n",
    "net.SGD(training_data, 8, mini_batch_size, 0.1, \n",
    "        validation_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cPickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7caa946c371b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#添加卷积池化层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#与上一个方法相比，计算量会小一点点，精度也高一些\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvPoolLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullyConnectedLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Convolutional-Networks-homework-master/network3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Standard library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#import cPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'"
     ]
    }
   ],
   "source": [
    "###002\n",
    "\"\"\"\"\n",
    "    在输入层之后，添加卷积池化层，进行特征提取，权值共享减少参数量，池化\n",
    "    池化作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。\n",
    "    以5*5的感受野为基础\n",
    "    映射到第一个隐藏层（5*5的点映射到一个点，要求参数和偏置），隐藏层池化（2*2的点映射到一个点，特定的计算方法），激活函数， 再映射到sigmoid层（全连接，池化层的每个点都连接到输出点），最后由softmax产生真正的输出\n",
    "    在这个架构中，我们可以把卷积和混合层看作是在学习输⼊训练图像中的局部感受野\n",
    "    ⽽后⾯的全连接层则在⼀个更抽象的层次学习，从整个图像整合全局信息。这是⼀种常⻅的卷积神经⽹络模式\n",
    "# 优点：相比上种方法，计算量少一些，网络训练结果也会更好\n",
    "# 缺点：仍然不是最优的\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "\n",
    "\n",
    "#获取数据\n",
    "training_data, validation_data, test_data = network3.load_data_shared() \n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "#设置网络\n",
    "#有卷积层，设置了图片，过滤（特征映射），池化，输出层全连接，softmax激活输出\n",
    "#采用SGD算法计算\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), #卷积池化层\n",
    "                  filter_shape=(4, 1, 5, 5),  #采用4层maps，一张图片提取多个不同特征，比如RGB三种颜色，亮度等\n",
    "                  poolsize=(2, 2)), #2*2的池化\n",
    "    FullyConnectedLayer(n_in=4*12*12, n_out=100), #全连接层\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size) #输出层\n",
    "net.SGD(training_data, 8, mini_batch_size, 0.1,  #用SGD求最优值\n",
    "        validation_data, test_data)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bb8d817b3870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#两个卷积池化层\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#优点：与单卷积层网络相比，计算量更小了，在训练的一开始误差也更小，但是最终误差几乎一致\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvPoolLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullyConnectedLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Convolutional-Network/network3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Third-party libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "###003\n",
    "\"\"\"\"\n",
    "# 在之前网络的基础上插⼊第⼆个卷积–混合层。把它插在已有的卷积–混合层和全连接隐藏层之间\n",
    "# 继续使⽤⼀个 5 × 5 局部感受野，混合 2 × 2 的区域\n",
    "# 第⼆个卷积–混合层输⼊ 12 × 12 幅“图像”，其“像素”代表原始输⼊图像中\n",
    "# 特定的局部特征的存在（或不存在）。这⼀层输⼊是原始输⼊图像的另外⼀个版本\n",
    "# 个版本是经过抽象和凝缩过的，但是仍然有⼤量的空间结构\n",
    "# 优点：与单卷积层网络相比，计算量更小了，在训练的一开始误差也更小，但是最终误差几乎一致\n",
    "# 缺点：扔有过拟合的风险\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "\n",
    "#获取数据\n",
    "training_data, validation_data, test_data = network3.load_data_shared() \n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                  filter_shape=(4, 1, 5, 5), \n",
    "                  poolsize=(2, 2)),\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 4, 12, 12), \n",
    "                  filter_shape=(4, 4, 5, 5), \n",
    "                  poolsize=(2, 2)),\n",
    "    FullyConnectedLayer(n_in=4*4*4, n_out=100),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.1, \n",
    "        validation_data, test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'theano'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4568509acee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#两个卷积池化层，改变最后的激活函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#与上一个相比，计算量同样很小，但是精度有所提高\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetwork3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvPoolLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullyConnectedLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/Convolutional-Network/network3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Third-party libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'theano'"
     ]
    }
   ],
   "source": [
    "###004\n",
    "\"\"\"\"\n",
    "# 在两个卷积层的基础上，改变最后的激活函数为tanh函数\n",
    "# tanh 函数可能是⼀个⽐ S 型函数更好的激活函数，在这里进行尝试\n",
    "# 优点：相比原本的网络，换了tanh函数的误差会小一些\n",
    "# 缺点：仍有过拟合的风险\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "from network3 import ReLU\n",
    "\n",
    "#获取数据\n",
    "training_data, validation_data, test_data = network3.load_data_shared() \n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "\n",
    "\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                  filter_shape=(4, 1, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 4, 12, 12), \n",
    "                  filter_shape=(4, 4, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    FullyConnectedLayer(n_in=4*4*4, n_out=100, activation_fn=ReLU),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(training_data, 60, mini_batch_size, 0.03, \n",
    "        validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 0: validation accuracy 98.24%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.25%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 1: validation accuracy 98.29%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.38%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 2: validation accuracy 98.33%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.54%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 3: validation accuracy 98.36%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.49%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 4: validation accuracy 98.60%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 98.59%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 5: validation accuracy 98.44%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 6: validation accuracy 98.49%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 7: validation accuracy 98.45%\n",
      "Finished training network.\n",
      "Best validation accuracy of 98.60% obtained at iteration 124999\n",
      "Corresponding test accuracy of 98.59%\n"
     ]
    }
   ],
   "source": [
    "###005\n",
    "\"\"\"\"\n",
    "# 对数据进行拓展，测试改进后的结果\n",
    "# 扩展训练数据的⼀个简单的⽅法是将每个训练图像由⼀个像素来代替，⽆论是上⼀个像素，⼀个像素，左边⼀个像素，或右边⼀个像素\n",
    "# 优点：与原本的相比，使用扩展的数据集后，计算量变得大一些，但是误差也有所减少\n",
    "\"\"\"\" \n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "from network3 import ReLU\n",
    "  \n",
    "expanded_training_data,validation_data, test_data= network3.load_data_shared(\n",
    "    \"../data/mnist_expanded.pkl.gz\")\n",
    "\n",
    "\n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                  filter_shape=(4, 1, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 4, 12, 12), \n",
    "                  filter_shape=(4, 4, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    FullyConnectedLayer(n_in=4*4*4, n_out=100, activation_fn=ReLU),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(expanded_training_data, 8, mini_batch_size, 0.03, \n",
    "        validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-4-cbb574114772>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-cbb574114772>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    \"\"\"\"\u001b[0m\n\u001b[0m        \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "###006\n",
    "\"\"\"\"\n",
    "# 添加一个额外的全连接层，测试修改后的结果\n",
    "# 优点：与单全链接层相比，计算量有所减小，误差差不多\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "from network3 import ReLU\n",
    "\n",
    "#获取数据\n",
    "# training_data, validation_data, test_data = network3.load_data_shared() \n",
    "expanded_training_data,validation_data, test_data= network3.load_data_shared(\n",
    "    \"../data/mnist_expanded.pkl.gz\")\n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "\n",
    "\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                  filter_shape=(4, 1, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 4, 12, 12), \n",
    "                  filter_shape=(4, 4, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    FullyConnectedLayer(n_in=4*4*4, n_out=100, activation_fn=ReLU),\n",
    "    FullyConnectedLayer(n_in=100, n_out=100, activation_fn=ReLU),\n",
    "    SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
    "net.SGD(expanded_training_data, 30, mini_batch_size, 0.03, \n",
    "        validation_data, test_data, lmbda=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to run under a GPU.  If this is not desired, then modify network3.py\n",
      "to set the GPU flag to False.\n",
      "Training mini-batch number 0\n",
      "Training mini-batch number 1000\n",
      "Training mini-batch number 2000\n",
      "Training mini-batch number 3000\n",
      "Training mini-batch number 4000\n",
      "Training mini-batch number 5000\n",
      "Training mini-batch number 6000\n",
      "Training mini-batch number 7000\n",
      "Training mini-batch number 8000\n",
      "Training mini-batch number 9000\n",
      "Training mini-batch number 10000\n",
      "Training mini-batch number 11000\n",
      "Training mini-batch number 12000\n",
      "Training mini-batch number 13000\n",
      "Training mini-batch number 14000\n",
      "Training mini-batch number 15000\n",
      "Training mini-batch number 16000\n",
      "Training mini-batch number 17000\n",
      "Training mini-batch number 18000\n",
      "Training mini-batch number 19000\n",
      "Training mini-batch number 20000\n",
      "Training mini-batch number 21000\n",
      "Training mini-batch number 22000\n",
      "Training mini-batch number 23000\n",
      "Training mini-batch number 24000\n",
      "Epoch 0: validation accuracy 94.29%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 94.42%\n",
      "Training mini-batch number 25000\n",
      "Training mini-batch number 26000\n",
      "Training mini-batch number 27000\n",
      "Training mini-batch number 28000\n",
      "Training mini-batch number 29000\n",
      "Training mini-batch number 30000\n",
      "Training mini-batch number 31000\n",
      "Training mini-batch number 32000\n",
      "Training mini-batch number 33000\n",
      "Training mini-batch number 34000\n",
      "Training mini-batch number 35000\n",
      "Training mini-batch number 36000\n",
      "Training mini-batch number 37000\n",
      "Training mini-batch number 38000\n",
      "Training mini-batch number 39000\n",
      "Training mini-batch number 40000\n",
      "Training mini-batch number 41000\n",
      "Training mini-batch number 42000\n",
      "Training mini-batch number 43000\n",
      "Training mini-batch number 44000\n",
      "Training mini-batch number 45000\n",
      "Training mini-batch number 46000\n",
      "Training mini-batch number 47000\n",
      "Training mini-batch number 48000\n",
      "Training mini-batch number 49000\n",
      "Epoch 1: validation accuracy 93.62%\n",
      "Training mini-batch number 50000\n",
      "Training mini-batch number 51000\n",
      "Training mini-batch number 52000\n",
      "Training mini-batch number 53000\n",
      "Training mini-batch number 54000\n",
      "Training mini-batch number 55000\n",
      "Training mini-batch number 56000\n",
      "Training mini-batch number 57000\n",
      "Training mini-batch number 58000\n",
      "Training mini-batch number 59000\n",
      "Training mini-batch number 60000\n",
      "Training mini-batch number 61000\n",
      "Training mini-batch number 62000\n",
      "Training mini-batch number 63000\n",
      "Training mini-batch number 64000\n",
      "Training mini-batch number 65000\n",
      "Training mini-batch number 66000\n",
      "Training mini-batch number 67000\n",
      "Training mini-batch number 68000\n",
      "Training mini-batch number 69000\n",
      "Training mini-batch number 70000\n",
      "Training mini-batch number 71000\n",
      "Training mini-batch number 72000\n",
      "Training mini-batch number 73000\n",
      "Training mini-batch number 74000\n",
      "Epoch 2: validation accuracy 95.38%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 95.70%\n",
      "Training mini-batch number 75000\n",
      "Training mini-batch number 76000\n",
      "Training mini-batch number 77000\n",
      "Training mini-batch number 78000\n",
      "Training mini-batch number 79000\n",
      "Training mini-batch number 80000\n",
      "Training mini-batch number 81000\n",
      "Training mini-batch number 82000\n",
      "Training mini-batch number 83000\n",
      "Training mini-batch number 84000\n",
      "Training mini-batch number 85000\n",
      "Training mini-batch number 86000\n",
      "Training mini-batch number 87000\n",
      "Training mini-batch number 88000\n",
      "Training mini-batch number 89000\n",
      "Training mini-batch number 90000\n",
      "Training mini-batch number 91000\n",
      "Training mini-batch number 92000\n",
      "Training mini-batch number 93000\n",
      "Training mini-batch number 94000\n",
      "Training mini-batch number 95000\n",
      "Training mini-batch number 96000\n",
      "Training mini-batch number 97000\n",
      "Training mini-batch number 98000\n",
      "Training mini-batch number 99000\n",
      "Epoch 3: validation accuracy 95.72%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.12%\n",
      "Training mini-batch number 100000\n",
      "Training mini-batch number 101000\n",
      "Training mini-batch number 102000\n",
      "Training mini-batch number 103000\n",
      "Training mini-batch number 104000\n",
      "Training mini-batch number 105000\n",
      "Training mini-batch number 106000\n",
      "Training mini-batch number 107000\n",
      "Training mini-batch number 108000\n",
      "Training mini-batch number 109000\n",
      "Training mini-batch number 110000\n",
      "Training mini-batch number 111000\n",
      "Training mini-batch number 112000\n",
      "Training mini-batch number 113000\n",
      "Training mini-batch number 114000\n",
      "Training mini-batch number 115000\n",
      "Training mini-batch number 116000\n",
      "Training mini-batch number 117000\n",
      "Training mini-batch number 118000\n",
      "Training mini-batch number 119000\n",
      "Training mini-batch number 120000\n",
      "Training mini-batch number 121000\n",
      "Training mini-batch number 122000\n",
      "Training mini-batch number 123000\n",
      "Training mini-batch number 124000\n",
      "Epoch 4: validation accuracy 94.64%\n",
      "Training mini-batch number 125000\n",
      "Training mini-batch number 126000\n",
      "Training mini-batch number 127000\n",
      "Training mini-batch number 128000\n",
      "Training mini-batch number 129000\n",
      "Training mini-batch number 130000\n",
      "Training mini-batch number 131000\n",
      "Training mini-batch number 132000\n",
      "Training mini-batch number 133000\n",
      "Training mini-batch number 134000\n",
      "Training mini-batch number 135000\n",
      "Training mini-batch number 136000\n",
      "Training mini-batch number 137000\n",
      "Training mini-batch number 138000\n",
      "Training mini-batch number 139000\n",
      "Training mini-batch number 140000\n",
      "Training mini-batch number 141000\n",
      "Training mini-batch number 142000\n",
      "Training mini-batch number 143000\n",
      "Training mini-batch number 144000\n",
      "Training mini-batch number 145000\n",
      "Training mini-batch number 146000\n",
      "Training mini-batch number 147000\n",
      "Training mini-batch number 148000\n",
      "Training mini-batch number 149000\n",
      "Epoch 5: validation accuracy 94.83%\n",
      "Training mini-batch number 150000\n",
      "Training mini-batch number 151000\n",
      "Training mini-batch number 152000\n",
      "Training mini-batch number 153000\n",
      "Training mini-batch number 154000\n",
      "Training mini-batch number 155000\n",
      "Training mini-batch number 156000\n",
      "Training mini-batch number 157000\n",
      "Training mini-batch number 158000\n",
      "Training mini-batch number 159000\n",
      "Training mini-batch number 160000\n",
      "Training mini-batch number 161000\n",
      "Training mini-batch number 162000\n",
      "Training mini-batch number 163000\n",
      "Training mini-batch number 164000\n",
      "Training mini-batch number 165000\n",
      "Training mini-batch number 166000\n",
      "Training mini-batch number 167000\n",
      "Training mini-batch number 168000\n",
      "Training mini-batch number 169000\n",
      "Training mini-batch number 170000\n",
      "Training mini-batch number 171000\n",
      "Training mini-batch number 172000\n",
      "Training mini-batch number 173000\n",
      "Training mini-batch number 174000\n",
      "Epoch 6: validation accuracy 95.53%\n",
      "Training mini-batch number 175000\n",
      "Training mini-batch number 176000\n",
      "Training mini-batch number 177000\n",
      "Training mini-batch number 178000\n",
      "Training mini-batch number 179000\n",
      "Training mini-batch number 180000\n",
      "Training mini-batch number 181000\n",
      "Training mini-batch number 182000\n",
      "Training mini-batch number 183000\n",
      "Training mini-batch number 184000\n",
      "Training mini-batch number 185000\n",
      "Training mini-batch number 186000\n",
      "Training mini-batch number 187000\n",
      "Training mini-batch number 188000\n",
      "Training mini-batch number 189000\n",
      "Training mini-batch number 190000\n",
      "Training mini-batch number 191000\n",
      "Training mini-batch number 192000\n",
      "Training mini-batch number 193000\n",
      "Training mini-batch number 194000\n",
      "Training mini-batch number 195000\n",
      "Training mini-batch number 196000\n",
      "Training mini-batch number 197000\n",
      "Training mini-batch number 198000\n",
      "Training mini-batch number 199000\n",
      "Epoch 7: validation accuracy 96.14%\n",
      "This is the best validation accuracy to date.\n",
      "The corresponding test accuracy is 96.44%\n",
      "Finished training network.\n",
      "Best validation accuracy of 96.14% obtained at iteration 199999\n",
      "Corresponding test accuracy of 96.44%\n"
     ]
    }
   ],
   "source": [
    "###007\n",
    "\"\"\"\"\n",
    "# 使⽤⼀个组合的⽹络：⼀个简单的进⼀步提⾼性能的⽅法是创建⼏个神经⽹络，然后让它们投票来决定最好的分类\n",
    "# 优点：与原来的网络相比，提高了泛化误差\n",
    "# 缺点：与原来的网络相比，计算量变大了，误差也稍微变大了一些\n",
    "\"\"\"\"\n",
    "import network3\n",
    "from network3 import Network\n",
    "from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
    "from network3 import ReLU\n",
    "\n",
    "#获取数据\n",
    "#training_data, validation_data, test_data = network3.load_data_shared() \n",
    "expanded_training_data, validation_data, test_data = network3.load_data_shared(\n",
    "        \"../data/mnist_expanded.pkl.gz\")\n",
    "#设置minibatch\n",
    "mini_batch_size = 10\n",
    "\n",
    "net = Network([\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28), \n",
    "                  filter_shape=(4, 1, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    ConvPoolLayer(image_shape=(mini_batch_size, 4, 12, 12), \n",
    "                  filter_shape=(4, 4, 5, 5), \n",
    "                  poolsize=(2, 2), \n",
    "                  activation_fn=ReLU),\n",
    "    FullyConnectedLayer(\n",
    "        n_in=4*4*4, n_out=100, activation_fn=ReLU, p_dropout=0.5),\n",
    "    FullyConnectedLayer(\n",
    "        n_in=100, n_out=100, activation_fn=ReLU, p_dropout=0.5),\n",
    "    SoftmaxLayer(n_in=100, n_out=10, p_dropout=0.5)], \n",
    "    mini_batch_size)\n",
    "net.SGD(expanded_training_data, 8, mini_batch_size, 0.03, \n",
    "        validation_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
